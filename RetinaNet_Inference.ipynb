{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9190a135",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cv2, os, ast, time, math\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.ops import box_iou\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.models.detection import retinanet_resnet50_fpn_v2, RetinaNet_ResNet50_FPN_V2_Weights\n",
    "from mAP import mean_average_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12436b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train data with annotations: (4919, 4)\n",
      "shape of training data: (4427, 4)\n",
      "shape of validation data: (492, 4)\n",
      "Min number of bboxs per image: 1\n",
      "Max number of bboxs per image: 18\n",
      "Max width of all bboxs: 243\n",
      "Max height of all bboxs: 222\n",
      "Min width of all bboxs: 17\n",
      "Min height of all bboxs: 13\n",
      "1.47% of all aspect ratios greater than 2\n",
      "1.15% of all aspect ratios less than 0.25\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "base_dir = 'h:/Python/ob'\n",
    "train_csv = os.path.join(base_dir, \"train.csv\")\n",
    "train_df = pd.read_csv(train_csv)\n",
    "train_df[\"img_path\"] = os.path.join(base_dir, \"train_images\") + \"/video_\" + train_df.video_id.astype(str) + \"/\" + train_df.video_frame.astype(str) + \".jpg\"\n",
    "train_df[\"annotations\"] = train_df[\"annotations\"].apply(eval)\n",
    "train_df[\"a_count\"] = train_df[\"annotations\"].apply(len)\n",
    "train_df = train_df.drop(columns=['video_id', 'sequence', 'video_frame', 'sequence_frame'])\n",
    "train_df_positive = train_df[train_df['a_count'] != 0]\n",
    "train_df_positive= train_df_positive.reset_index(drop=True)\n",
    "print('shape of train data with annotations:', train_df_positive.shape)\n",
    "\n",
    "train_df_ratio = (train_df_positive.set_index('image_id').explode('annotations').\n",
    "                  apply(lambda row: pd.Series(row['annotations']), axis=1).reset_index())\n",
    "train_df_ratio['aspect_ratio'] = train_df_ratio['height']/train_df_ratio['width']\n",
    "\n",
    "\n",
    "train_df_p, val_df_p = train_test_split(train_df_positive, test_size=0.1, random_state=0)\n",
    "print('shape of training data:', train_df_p.shape)\n",
    "print('shape of validation data:', val_df_p.shape)\n",
    "print('Min number of bboxs per image:', min(train_df_positive.a_count))\n",
    "print('Max number of bboxs per image:', max(train_df_positive.a_count))\n",
    "print('Max width of all bboxs:', max(train_df_ratio.width))\n",
    "print('Max height of all bboxs:', max(train_df_ratio.height))\n",
    "print('Min width of all bboxs:', min(train_df_ratio.width))\n",
    "print('Min height of all bboxs:', min(train_df_ratio.height))\n",
    "\n",
    "print('{:.2%} of all aspect ratios greater than 2'.\n",
    "      format(train_df_ratio[train_df_ratio['aspect_ratio'] > 1.5].shape[0]/train_df_ratio.shape[0]))\n",
    "\n",
    "print('{:.2%} of all aspect ratios less than 0.25'.\n",
    "      format(train_df_ratio[train_df_ratio['aspect_ratio'] < 0.5].shape[0]/train_df_ratio.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bd7593c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class COTS_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df_img, df_bbox, original_size=(1280, 720), resize_size=(1280, 720)):\n",
    "        self.df_img = df_img\n",
    "        self.df_bbox = df_bbox\n",
    "        self.orginal_size = original_size\n",
    "        self.resize_size = resize_size\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # load images and masks\n",
    "        row = self.df_img.iloc[idx]\n",
    "        img = Image.open(row['img_path']).convert('RGB')\n",
    "        if True: \n",
    "            img = np.array(img)/255.\n",
    "        else:\n",
    "            img = np.array(img.resize((self.w, self.h), resample=Image.BILINEAR))/255.\n",
    "        data = self.df_bbox[self.df_bbox['image_id'] == row['image_id']]\n",
    "        labels = ['cots'] * data.shape[0]\n",
    "        data = data[['x','y','width','height']].values\n",
    "        area = data[:, 2] * data[:, 3]\n",
    "        data[:,[2]] += data[:,[0]]\n",
    "        data[:,[3]] += data[:,[1]]\n",
    "        boxes = data.astype(np.uint32).tolist() # convert to absolute coordinates\n",
    "        # torch FRCNN expects ground truths as a dictionary of tensors\n",
    "        iscrowd = torch.zeros((data.shape[0],), dtype=torch.int64)\n",
    "        target = {}\n",
    "        target[\"boxes\"] = torch.Tensor(boxes).float()\n",
    "        target[\"labels\"] = torch.Tensor([1 for i in labels]).long()\n",
    "        target[\"image_id\"] = torch.Tensor([idx])\n",
    "        target[\"area\"] = torch.Tensor(area).float()\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        img = torch.tensor(img).permute(2,0,1)\n",
    "        return img.to(device).float(), target\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        return tuple(zip(*batch)) \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df_img.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "737c384d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = COTS_Dataset(train_df_p, train_df_ratio)\n",
    "test_ds = COTS_Dataset(val_df_p, train_df_ratio)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=1, collate_fn=train_ds.collate_fn, drop_last=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=1, collate_fn=test_ds.collate_fn, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59125c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "num_classes = 2\n",
    "model = retinanet_resnet50_fpn_v2(weights=RetinaNet_ResNet50_FPN_V2_Weights.COCO_V1)\n",
    "# replace classification layer \n",
    "out_channels = model.head.classification_head.conv[0].out_channels\n",
    "num_anchors = model.head.classification_head.num_anchors\n",
    "model.head.classification_head.num_classes = num_classes\n",
    "\n",
    "cls_logits = torch.nn.Conv2d(out_channels, num_anchors * num_classes, kernel_size = 3, stride=1, padding=1)\n",
    "torch.nn.init.normal_(cls_logits.weight, std=0.01)  # as per pytorch code\n",
    "torch.nn.init.constant_(cls_logits.bias, -math.log((1 - 0.01) / 0.01))  # as per pytorcch code \n",
    "# assign cls head to model\n",
    "model.head.classification_head.cls_logits = cls_logits\n",
    "\n",
    "model.load_state_dict(torch.load('./retinanet_resnet50_fpn_anchor_mean_std_e12.bin'))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "from torchvision.ops import nms\n",
    "def decode_output(output):\n",
    "    'convert tensors to numpy arrays'\n",
    "    bbs = output['boxes'].cpu().detach().numpy().astype(np.uint16)\n",
    "    labels = np.array(['cots' for i in output['labels'].cpu().detach().numpy()])\n",
    "    confs = output['scores'].cpu().detach().numpy()\n",
    "    ixs = nms(torch.tensor(bbs.astype(np.float32)), torch.tensor(confs), 0.5)\n",
    "    bbs, confs, labels = [tensor[ixs] for tensor in [bbs, confs, labels]]\n",
    "\n",
    "    if len(ixs) == 1:\n",
    "        bbs, confs, labels = [np.array([tensor]) for tensor in [bbs, confs, labels]]\n",
    "    return bbs.tolist(), confs.tolist(), labels.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2448878a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score(preds: List[torch.Tensor], gts: List[torch.Tensor], iou_th: float) -> float:\n",
    "    num_tp = 0\n",
    "    num_fp = 0\n",
    "    num_fn = 0\n",
    "    for p, GT in zip(preds, gts):\n",
    "        if len(p) and len(GT):\n",
    "            gt = GT.clone()\n",
    "            gt[:, 2] = gt[:, 0] + gt[:, 2]\n",
    "            gt[:, 3] = gt[:, 1] + gt[:, 3]\n",
    "            pp = p.clone()\n",
    "            pp[:, 2] = pp[:, 0] + pp[:, 2]\n",
    "            pp[:, 3] = pp[:, 1] + pp[:, 3]\n",
    "            iou_matrix = box_iou(pp, gt)\n",
    "            tp = len(torch.where(iou_matrix.max(0)[0] >= iou_th)[0])\n",
    "            fp = len(p) - tp\n",
    "            fn = len(torch.where(iou_matrix.max(0)[0] < iou_th)[0])\n",
    "            num_tp += tp\n",
    "            num_fp += fp\n",
    "            num_fn += fn\n",
    "        elif len(p) == 0 and len(GT):\n",
    "            num_fn += len(GT)\n",
    "        elif len(p) and len(GT) == 0:\n",
    "            num_fp += len(p)\n",
    "    score = 5 * num_tp / (5 * num_tp + 4 * num_fn + num_fp)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dfdcc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP on training set is: tensor(0.3639)\n",
      "mAP on training set is: tensor(0.3673)\n"
     ]
    }
   ],
   "source": [
    "# mAP score on training data and test data\n",
    "pred_boxes_map = []\n",
    "true_boxes_map = []\n",
    "\n",
    "for ix, (image, targets) in enumerate(train_loader):\n",
    "    outputs = model(image)[0]\n",
    "    bbs, confs, labels = decode_output(outputs)\n",
    "    for i in range(len(bbs)): \n",
    "        a = [targets[0]['image_id'].cpu().detach().item(), 0, confs[0]] + bbs[i]\n",
    "        pred_boxes_map.append(a)\n",
    "        \n",
    "    gt_bbs = targets[0]['boxes'].cpu().detach().numpy().astype(np.uint16).tolist()\n",
    "    for j in range(len(gt_bbs)): \n",
    "        b = [targets[0]['image_id'].cpu().detach().item(), 0, 1] + gt_bbs[j]\n",
    "        true_boxes_map.append(b)\n",
    "        \n",
    "mean_ap_train = mean_average_precision(pred_boxes_map, true_boxes_map, \n",
    "                                       iou_threshold=0.5, box_format=\"midpoint\", num_classes=1)\n",
    "print('mAP on training set is:', mean_ap_train)\n",
    "\n",
    "pred_boxes_map = []\n",
    "true_boxes_map = []\n",
    "\n",
    "for ix, (image, targets) in enumerate(test_loader):\n",
    "    outputs = model(image)[0]\n",
    "    bbs, confs, labels = decode_output(outputs)\n",
    "    for i in range(len(bbs)): \n",
    "        a = [targets[0]['image_id'].cpu().detach().item(), 0, confs[0]] + bbs[i]\n",
    "        pred_boxes_map.append(a)\n",
    "        \n",
    "    gt_bbs = targets[0]['boxes'].cpu().detach().numpy().astype(np.uint16).tolist()\n",
    "    for j in range(len(gt_bbs)): \n",
    "        b = [targets[0]['image_id'].cpu().detach().item(), 0, 1] + gt_bbs[j]\n",
    "        true_boxes_map.append(b)\n",
    "        \n",
    "mean_ap_test = mean_average_precision(pred_boxes_map, true_boxes_map, \n",
    "                                      iou_threshold=0.5, box_format=\"midpoint\", num_classes=1)\n",
    "print('mAP on training set is:', mean_ap_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a34f7509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Time Used: 802.76 seconds\n",
      "Avg inference time for training set: 0.09071632236181706\n",
      "Training average F2 score is 0.731\n",
      "Inference Time Used: 88.26 seconds\n",
      "Avg inference time for validation set: 0.09062939882278442\n",
      "Test average F2 score is 0.725\n"
     ]
    }
   ],
   "source": [
    "# F2 score on training data and test data\n",
    "predictions = []\n",
    "gts = []\n",
    "infer_time_train = []\n",
    "\n",
    "time_begin = time.time()\n",
    "for ix, (image, targets) in enumerate(train_loader):\n",
    "    t0 = time.time() \n",
    "    outputs = model(image)[0]\n",
    "    infer_time_train.append(time.time() - t0)\n",
    "    gts.append(targets[0]['boxes']) \n",
    "    predictions.append(outputs['boxes'].cpu().detach())\n",
    "    \n",
    "print('Inference Time Used: {:.2f} seconds'.format(time.time()- time_begin))\n",
    "iou_ths = np.arange(0.3, 0.85, 0.05)\n",
    "scores = [calculate_score(predictions, gts, iou_th) for iou_th in iou_ths]\n",
    "print('Avg inference time for training set:', np.mean(infer_time_train))\n",
    "print('Training average F2 score is {:.3f}'.format(np.mean(scores))) \n",
    "\n",
    "predictions = []\n",
    "gts = []\n",
    "infer_time_val = []\n",
    "\n",
    "time_begin = time.time()\n",
    "for ix, (image, targets) in enumerate(test_loader):\n",
    "    t0 = time.time() \n",
    "    outputs = model(image)[0]\n",
    "    infer_time_val.append(time.time() - t0)\n",
    "    gts.append(targets[0]['boxes']) \n",
    "    predictions.append(outputs['boxes'].cpu().detach())\n",
    "    \n",
    "print('Inference Time Used: {:.2f} seconds'.format(time.time()- time_begin))\n",
    "iou_ths = np.arange(0.3, 0.85, 0.05)\n",
    "scores = [calculate_score(predictions, gts, iou_th) for iou_th in iou_ths]\n",
    "print('Avg inference time for validation set:', np.mean(infer_time_val))\n",
    "print('Test average F2 score is {:.3f}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "102b75ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bbox(annots):\n",
    "    bboxes = [list(annot.values()) for annot in annots]\n",
    "    if bboxes == []:\n",
    "        return []\n",
    "    else:\n",
    "        bboxes = np.array(bboxes)\n",
    "        bboxes[:,[2]] += bboxes[:,[0]]\n",
    "        bboxes[:,[3]] += bboxes[:,[1]]\n",
    "        return bboxes.tolist()\n",
    "\n",
    "train_df['bboxes'] = train_df.annotations.apply(get_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285b6ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F2 score on all data including negative images \n",
    "predictions = []\n",
    "gts = []\n",
    "\n",
    "time_begin = time.time()\n",
    "for idx, row in train_df.iterrows():\n",
    "    img = Image.open(row['img_path']).convert('RGB')\n",
    "    img = np.array(img)/255.\n",
    "    img = torch.tensor(img).permute(2,0,1)\n",
    "    outputs = model((img.float().to(device),))[0]\n",
    "    pred = outputs['boxes'].cpu().detach()\n",
    "    if pred.size() == torch.Size([0, 4]):\n",
    "        predictions.append(torch.tensor([]).cpu().detach())\n",
    "    else:\n",
    "        predictions.append(pred)\n",
    "    gts.append(torch.tensor(row.bboxes))\n",
    "    \n",
    "print('Inference Time Used: {:.2f} seconds'.format(time.time()- time_begin))\n",
    "iou_ths = np.arange(0.3, 0.85, 0.05)\n",
    "scores = [calculate_score(predictions, gts, iou_th) for iou_th in iou_ths]\n",
    "print('Test average F2 score is {:.3f}'.format(np.mean(scores)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d34529e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on images \n",
    "random_sample = np.random.randint(0, 491, size=4)\n",
    "\n",
    "fig, ax = plt.subplots(4, 1, figsize=(100, 50))\n",
    "\n",
    "for i in range(len(random_sample)):\n",
    "    image, targets =  test_ds[random_sample[i-1]]\n",
    "    sample = (image.permute(1,2,0).cpu().numpy()*255).astype(np.int32)\n",
    "    boxes = targets['boxes'].cpu().numpy().astype(np.int32)\n",
    "    \n",
    "    pred_box = bboxconda [random_sample[i-1]]\n",
    "    \n",
    "    # Red for ground truth\n",
    "    for box in boxes:\n",
    "        cv2.rectangle(sample,(box[0], box[1]),(box[2], box[3]),(255, 0, 0), 3)\n",
    "\n",
    "\n",
    "    # Green for predictions\n",
    "    for box in pred_box:\n",
    "        cv2.rectangle(sample,(box[0], box[1]),(box[2], box[3]),(0, 255, 0), 3)\n",
    "    ax[i-1].set_axis_off()\n",
    "    ax[i-1].imshow(sample)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
