{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0399fd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13.0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import cv2, os, ast, time, math, shutil, sys, glob \n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas() \n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.ops import box_iou\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "654a8757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coco2yolo(image_height, image_width, bboxes):\n",
    "    \"\"\"\n",
    "    coco => [xmin, ymin, w, h]\n",
    "    yolo => [xmid, ymid, w, h] (normalized)\n",
    "    \"\"\"\n",
    "    \n",
    "    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n",
    "    \n",
    "    for bbox in bboxes:\n",
    "        if bbox[0] + bbox[2] >= image_width:\n",
    "            bbox[2] = image_width - bbox[0] - 1\n",
    "        if bbox[1] + bbox[3] >= image_height:\n",
    "            bbox[3] = image_height - bbox[1] - 1\n",
    "    \n",
    "    # normolizinig\n",
    "    bboxes[..., [0, 2]]= bboxes[..., [0, 2]]/ image_width\n",
    "    bboxes[..., [1, 3]]= bboxes[..., [1, 3]]/ image_height\n",
    "    \n",
    "    # converstion (xmin, ymin) => (xmid, ymid)\n",
    "    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]/2\n",
    "    \n",
    "    return bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "90352fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h:\\Python\\ob\n",
      "h:\\Python\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "cwd = os.getcwd()\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6be73335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train data with annotations: (4919, 4)\n",
      "shape of training data: (4427, 4)\n",
      "shape of validation data: (492, 4)\n",
      "Min number of bboxs per image: 1\n",
      "Max number of bboxs per image: 18\n",
      "Max width of all bboxs: 243\n",
      "Max height of all bboxs: 222\n",
      "Min width of all bboxs: 17\n",
      "Min height of all bboxs: 13\n",
      "1.47% of all aspect ratios greater than 2\n",
      "1.15% of all aspect ratios less than 0.25\n"
     ]
    }
   ],
   "source": [
    "base_dir = cwd\n",
    "train_csv = os.path.join(base_dir, \"train.csv\")\n",
    "train_df = pd.read_csv(train_csv)\n",
    "train_df[\"img_path\"] = os.path.join(base_dir, \"train_images\") + \"/video_\" + train_df.video_id.astype(str) + \"/\" + train_df.video_frame.astype(str) + \".jpg\"\n",
    "train_df[\"annotations\"] = train_df[\"annotations\"].apply(eval)\n",
    "train_df[\"a_count\"] = train_df[\"annotations\"].apply(len)\n",
    "train_df = train_df.drop(columns=['video_id', 'sequence', 'video_frame', 'sequence_frame'])\n",
    "train_df_positive = train_df[train_df['a_count'] != 0]\n",
    "train_df_positive= train_df_positive.reset_index(drop=True)\n",
    "print('shape of train data with annotations:', train_df_positive.shape)\n",
    "\n",
    "train_df_ratio = (train_df_positive.set_index('image_id').explode('annotations').\n",
    "                  apply(lambda row: pd.Series(row['annotations']), axis=1).reset_index())\n",
    "train_df_ratio['aspect_ratio'] = train_df_ratio['height']/train_df_ratio['width']\n",
    "\n",
    "\n",
    "train_df_p, val_df_p = train_test_split(train_df_positive, test_size=0.1, random_state=0)\n",
    "print('shape of training data:', train_df_p.shape)\n",
    "print('shape of validation data:', val_df_p.shape)\n",
    "print('Min number of bboxs per image:', min(train_df_positive.a_count))\n",
    "print('Max number of bboxs per image:', max(train_df_positive.a_count))\n",
    "print('Max width of all bboxs:', max(train_df_ratio.width))\n",
    "print('Max height of all bboxs:', max(train_df_ratio.height))\n",
    "print('Min width of all bboxs:', min(train_df_ratio.width))\n",
    "print('Min height of all bboxs:', min(train_df_ratio.height))\n",
    "\n",
    "print('{:.2%} of all aspect ratios greater than 2'.\n",
    "      format(train_df_ratio[train_df_ratio['aspect_ratio'] > 1.5].shape[0]/train_df_ratio.shape[0]))\n",
    "\n",
    "print('{:.2%} of all aspect ratios less than 0.25'.\n",
    "      format(train_df_ratio[train_df_ratio['aspect_ratio'] < 0.5].shape[0]/train_df_ratio.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67df9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bbox(annots):\n",
    "    bboxes = [list(annot.values()) for annot in annots]\n",
    "    return bboxes\n",
    "\n",
    "train_df_p['bboxes'] = train_df.annotations.apply(get_bbox)\n",
    "val_df_p['bboxes'] = train_df.annotations.apply(get_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2219dd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd train_models/\n",
    "cwd = os.getcwd()\n",
    "\n",
    "IMAGE_DIR = \"/images\"\n",
    "LABEL_DIR = \"/labels\"\n",
    "\n",
    "os.makedirs(cwd + '/images')\n",
    "os.makedirs(cwd + '/labels')\n",
    "\n",
    "os.makedirs(cwd + '/images/train')\n",
    "os.makedirs(cwd + '/labels/train')\n",
    "os.makedirs(cwd + '/images/val')\n",
    "os.makedirs(cwd + '/labels/val')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c77956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeLabels(bboxes, destfile, image_width = 1280, image_height = 720):\n",
    "    bboxes_coco  = np.array(bboxes).astype(np.float32).copy()\n",
    "    num_bbox     = len(bboxes_coco)\n",
    "    names        = ['cots'] * num_bbox\n",
    "    labels       = [0] * num_bbox\n",
    "    with open(destfile, 'w') as f:\n",
    "        if num_bbox<1:\n",
    "            annot = ''\n",
    "            f.write(annot)\n",
    "        else:\n",
    "            bboxes_yolo  = coco2yolo(image_height, image_width, bboxes_coco)\n",
    "            for bbox_idx in range(len(bboxes_yolo)):\n",
    "                annot = [str(labels[bbox_idx])]+ list(bboxes_yolo[bbox_idx].astype(str))+(['\\n'] if num_bbox!=(bbox_idx+1) else [''])\n",
    "                annot = ' '.join(annot)\n",
    "                annot = annot.strip(' ')\n",
    "                f.write(annot)\n",
    "    return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c465bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row_idx in tqdm(range(len(train_df_p))):\n",
    "    row = train_df_p.iloc[row_idx]\n",
    "    shutil.copyfile(row.img_path, f'{cwd}/images/train/{row.image_id}.jpg')\n",
    "    writeLabels(row.bboxes, f'{cwd}/labels/train/{row.image_id}.txt')\n",
    "\n",
    "for row_idx in tqdm(range(len(val_df_p))):\n",
    "    row = val_df_p.iloc[row_idx]\n",
    "    shutil.copyfile(row.img_path, f'{cwd}/images/val/{row.image_id}.jpg')\n",
    "    writeLabels(row.bboxes, f'{cwd}/labels/val/{row.image_id}.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f11b7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = []\n",
    "cots_dir = os.path.join(base_dir, 'train_models/YOLO/yolov7/cots')\n",
    "os.chdir(cots_dir)\n",
    "for file in glob.glob(os.path.join(cots_dir, 'images/train/*.jpg')):\n",
    "    file_path.append(file)\n",
    "\n",
    "textfile = open(\"./train.txt\", \"w\")\n",
    "for element in file_path:\n",
    "    textfile.write(element + \"\\n\")\n",
    "textfile.close()\n",
    "\n",
    "\n",
    "file_path = []\n",
    "for file in glob.glob(os.path.join(cots_dir, 'images/val/*.jpg')):\n",
    "    file_path.append(file)\n",
    "\n",
    "textfile = open(\"./val.txt\", \"w\")\n",
    "for element in file_path:\n",
    "    textfile.write(element + \"\\n\")\n",
    "textfile.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
