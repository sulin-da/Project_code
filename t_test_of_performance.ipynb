{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0399fd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import cv2, os, ast, time, math, shutil, sys, glob, re\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas() \n",
    "from joblib import Parallel, delayed\n",
    "from scipy import stats\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.ops import box_iou\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib.image import imread\n",
    "from typing import List\n",
    "from mAP import mean_average_precision \n",
    "from torchinfo import summary\n",
    "from torchvision.models.detection import retinanet_resnet50_fpn_v2, RetinaNet_ResNet50_FPN_V2_Weights\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5702d7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h:\\Python\\ob\\train_models\\YOLO\\yolov7\n"
     ]
    }
   ],
   "source": [
    "%cd yolov7\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3dd2fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'h:/Python/ob/train_models'\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "ckpt_path_model_1 = base_dir + '/YOLO/runs_yolov7_1280_720/weights/best.pt'\n",
    "ckpt_path_model_2 = base_dir + '/FasterRCNN/fasterrcnn_resnet50_fpn_free_last2_batch3_e12.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cb23cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.experimental import attempt_load \n",
    "from utils.datasets import LoadImages\n",
    "from utils.general import check_img_size, check_requirements, check_imshow, non_max_suppression\n",
    "model_1 = attempt_load(ckpt_path_model_1, map_location=device)\n",
    "model_1.eval()\n",
    "\n",
    "'''\n",
    "model_1 = torchvision.models.detection.fasterrcnn_resnet50_fpn()\n",
    "in_features = model_1.roi_heads.box_predictor.cls_score.in_features\n",
    "model_1.roi_heads.box_predictor = FastRCNNPredictor(in_features, 2)\n",
    "\n",
    "model_1.load_state_dict(torch.load(ckpt_path_model_1))\n",
    "model_1.to(device)\n",
    "model_1.eval()\n",
    "'''\n",
    "\n",
    "model_2 = torchvision.models.detection.fasterrcnn_resnet50_fpn()\n",
    "in_features = model_2.roi_heads.box_predictor.cls_score.in_features\n",
    "model_2.roi_heads.box_predictor = FastRCNNPredictor(in_features, 2)\n",
    "\n",
    "model_2.load_state_dict(torch.load(ckpt_path_model_2))\n",
    "model_2.to(device)\n",
    "model_2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99ae8153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_cots(img_path, model, img_size, stride, device): \n",
    "    \n",
    "    dataset = LoadImages(img_path, img_size=img_size, stride=stride)\n",
    "\n",
    "    for path, img, im0s, vid_cap in dataset:\n",
    "        img = torch.from_numpy(img).to(device).float()\n",
    "        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "        if img.ndimension() == 3:\n",
    "            img = img.unsqueeze(0)\n",
    "    # Inference\n",
    "        #print(im0s)\n",
    "        #print(img[:, 2, 100, 500])\n",
    "        t0 = time.time()\n",
    "        pred = model(img, augment=None)[0]\n",
    "        pred = non_max_suppression(pred) #opt.iou_thres, classes=opt.classes, agnostic=opt.agnostic_nms)\n",
    "        inference_time = time.time() - t0\n",
    "        del img\n",
    "        torch.cuda.empty_cache()\n",
    "    return pred, inference_time\n",
    "\n",
    "def yolo2xy(bboxes, height=720, width=1280):\n",
    "    \"\"\"\n",
    "    yolo => [xmid, ymid, w, h] (normalized)\n",
    "    coco => [xmin, ymin, w, h]\n",
    "    \n",
    "    \"\"\" \n",
    "    # denormalizing\n",
    "    bboxes[..., 0::2] *= width\n",
    "    bboxes[..., 1::2] *= height\n",
    "    \n",
    "    # converstion (xmid, ymid) => (xmin, ymin) \n",
    "    bboxes[..., 0:2] -= bboxes[..., 2:4]/2\n",
    "    bboxes[:,[2]] += bboxes[:,[0]]\n",
    "    bboxes[:,[3]] += bboxes[:,[1]]\n",
    "    \n",
    "    return bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ff4b85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paired t-test on detection speed \n",
    "df_val = pd.read_csv('./cots/val.txt', header=None, names=['img_path'])\n",
    "df_train = pd.read_csv('./cots/train.txt', header=None, names=['img_path'])\n",
    "col_names={0:'class', 1:'x_mid', 2:'y_mid', 3:'w_ratio', 4:'h_ratio'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7579f81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score(preds: List[torch.Tensor], gts: List[torch.Tensor], iou_th: float) -> float:\n",
    "    num_tp = 0\n",
    "    num_fp = 0\n",
    "    num_fn = 0\n",
    "    for p, GT in zip(preds, gts):\n",
    "        if len(p) and len(GT):\n",
    "            gt = GT.clone()\n",
    "            gt[:, 2] = gt[:, 0] + gt[:, 2]\n",
    "            gt[:, 3] = gt[:, 1] + gt[:, 3]\n",
    "            pp = p.clone()\n",
    "            pp[:, 2] = pp[:, 0] + pp[:, 2]\n",
    "            pp[:, 3] = pp[:, 1] + pp[:, 3]\n",
    "            iou_matrix = box_iou(pp, gt)\n",
    "            tp = len(torch.where(iou_matrix.max(0)[0] >= iou_th)[0])\n",
    "            fp = len(p) - tp\n",
    "            fn = len(torch.where(iou_matrix.max(0)[0] < iou_th)[0])\n",
    "            num_tp += tp\n",
    "            num_fp += fp\n",
    "            num_fn += fn\n",
    "        elif len(p) == 0 and len(GT):\n",
    "            num_fn += len(GT)\n",
    "        elif len(p) and len(GT) == 0:\n",
    "            num_fp += len(p)\n",
    "    score = 5 * num_tp / (5 * num_tp + 4 * num_fn + num_fp)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8361d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sl4n21\\Miniconda3\\envs\\myenv\\lib\\site-packages\\torch\\functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:2895.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Time Used: 780.95 seconds\n",
      "Inference Time Used: 84.16 seconds\n"
     ]
    }
   ],
   "source": [
    "# F2 score on training data and test data for model_1\n",
    "\n",
    "infer_time_train_1 = {}\n",
    "f2_value_train_1 = {}\n",
    "iou_ths = np.arange(0.3, 0.85, 0.05)\n",
    "time_begin = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(df_train.shape[0]):\n",
    "        preds = []\n",
    "        gts = []\n",
    "        img_path = df_train.img_path[i]\n",
    "        img_id = re.search('(\\d-.+?).jpg', img_path).group(1)\n",
    "        label_path = img_path.replace('jpg', 'txt')\n",
    "        label_path = label_path.replace('images', 'labels')\n",
    "        pred, infer_time = detect_cots(img_path, model=model_1, img_size=1280, stride=32, device=device) \n",
    "        pred = pred[0].cpu().detach()\n",
    "        pred_box = pred[:, 0:4]\n",
    "        preds.append(pred_box)\n",
    "        infer_time_train_1[img_id] = infer_time\n",
    "        bbox = pd.read_csv(label_path, sep=' ', header=None)\n",
    "        bbox = bbox.rename(columns=col_names)\n",
    "        bbox = bbox[['x_mid', 'y_mid', 'w_ratio', 'h_ratio']]\n",
    "        gt_box = yolo2xy(bbox.to_numpy()).astype(np.int32)\n",
    "        gts.append(torch.tensor(gt_box))\n",
    "        scores = np.mean([calculate_score(preds, gts, iou_th) for iou_th in iou_ths])\n",
    "        f2_value_train_1[img_id] = scores\n",
    "        del pred\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print('Inference Time Used: {:.2f} seconds'.format(time.time()- time_begin))\n",
    "\n",
    "infer_time_val_1 = {}\n",
    "f2_value_val_1 = {}\n",
    "time_begin = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(df_val.shape[0]):\n",
    "        preds = []\n",
    "        gts = []\n",
    "        img_path = df_val.img_path[i]\n",
    "        img_id = re.search('(\\d-.+?).jpg', img_path).group(1)\n",
    "        label_path = img_path.replace('jpg', 'txt')\n",
    "        label_path = label_path.replace('images', 'labels')\n",
    "        pred, infer_time = detect_cots(img_path, model=model_1, img_size=1280, stride=32, device=device) \n",
    "        pred = pred[0].cpu().detach()\n",
    "        pred_box = pred[:, 0:4]\n",
    "        preds.append(pred_box)\n",
    "        infer_time_val_1[img_id] = infer_time\n",
    "        bbox = pd.read_csv(label_path, sep=' ', header=None)\n",
    "        bbox = bbox.rename(columns=col_names)\n",
    "        bbox = bbox[['x_mid', 'y_mid', 'w_ratio', 'h_ratio']]\n",
    "        gt_box = yolo2xy(bbox.to_numpy()).astype(np.int32)\n",
    "        gts.append(torch.tensor(gt_box))\n",
    "        scores = np.mean([calculate_score(preds, gts, iou_th) for iou_th in iou_ths])\n",
    "        f2_value_val_1[img_id] = scores\n",
    "        del pred\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print('Inference Time Used: {:.2f} seconds'.format(time.time()- time_begin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "420f2a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'h:\\Python\\ob'\n",
    "train_csv = os.path.join(base_dir, \"train.csv\")\n",
    "train_df = pd.read_csv(train_csv)\n",
    "train_df[\"img_path\"] = os.path.join(base_dir, \"train_images\") + \"/video_\" + train_df.video_id.astype(str) + \"/\" + train_df.video_frame.astype(str) + \".jpg\"\n",
    "train_df[\"annotations\"] = train_df[\"annotations\"].apply(eval)\n",
    "train_df[\"a_count\"] = train_df[\"annotations\"].apply(len)\n",
    "train_df = train_df.drop(columns=['video_id', 'sequence', 'video_frame', 'sequence_frame'])\n",
    "train_df_positive = train_df[train_df['a_count'] != 0]\n",
    "train_df_positive= train_df_positive.reset_index(drop=True)\n",
    "\n",
    "train_df_ratio = (train_df_positive.set_index('image_id').explode('annotations').\n",
    "                  apply(lambda row: pd.Series(row['annotations']), axis=1).reset_index())\n",
    "train_df_ratio['aspect_ratio'] = train_df_ratio['height']/train_df_ratio['width']\n",
    "\n",
    "\n",
    "train_df_p, val_df_p = train_test_split(train_df_positive, test_size=0.1, random_state=0)\n",
    "\n",
    "class COTS_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df_img, df_bbox, original_size=(1280, 720), resize_size=(1280, 720)):\n",
    "        self.df_img = df_img\n",
    "        self.df_bbox = df_bbox\n",
    "        self.orginal_size = original_size\n",
    "        self.resize_size = resize_size\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # load images and masks\n",
    "        row = self.df_img.iloc[idx]\n",
    "        img = Image.open(row['img_path']).convert('RGB')\n",
    "        if True: \n",
    "            img = np.array(img)/255.\n",
    "        else:\n",
    "            img = np.array(img.resize((self.w, self.h), resample=Image.BILINEAR))/255.\n",
    "        data = self.df_bbox[self.df_bbox['image_id'] == row['image_id']]\n",
    "        labels = ['cots'] * data.shape[0]\n",
    "        data = data[['x','y','width','height']].values\n",
    "        area = data[:, 2] * data[:, 3]\n",
    "        data[:,[2]] += data[:,[0]]\n",
    "        data[:,[3]] += data[:,[1]]\n",
    "        boxes = data.astype(np.uint32).tolist() # convert to absolute coordinates\n",
    "        # torch FRCNN expects ground truths as a dictionary of tensors\n",
    "        iscrowd = torch.zeros((data.shape[0],), dtype=torch.int64)\n",
    "        target = {}\n",
    "        target[\"boxes\"] = torch.Tensor(boxes).float()\n",
    "        target[\"labels\"] = torch.Tensor([1 for i in labels]).long()\n",
    "        target[\"image_id\"] = row['image_id']\n",
    "        target[\"area\"] = torch.Tensor(area).float()\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        img = torch.tensor(img).permute(2,0,1)\n",
    "        return img.to(device).float(), target\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        return tuple(zip(*batch)) \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df_img.shape[0]\n",
    "\n",
    "train_ds = COTS_Dataset(train_df_p, train_df_ratio)\n",
    "test_ds = COTS_Dataset(val_df_p, train_df_ratio)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=1, collate_fn=train_ds.collate_fn, drop_last=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=1, collate_fn=test_ds.collate_fn, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69f2e94c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# F2 score on training data and test data for model_1, if both are Faster RCNN\\ninfer_time_train_1 = {}\\nf2_value_train_1 = {}\\niou_ths = np.arange(0.3, 0.85, 0.05)\\n\\ntime_begin = time.time()\\nfor ix, (image, targets) in enumerate(train_loader):\\n    preds = []\\n    gts = []\\n    t0 = time.time() \\n    outputs = model_1(image)[0]\\n    infe_time = time.time() - t0 \\n    infer_time_train_1[targets[0]['image_id']] = infe_time\\n    gts.append(targets[0]['boxes']) \\n    preds.append(outputs['boxes'].cpu().detach())\\n    scores = [calculate_score(preds, gts, iou_th) for iou_th in iou_ths]\\n    f2_value_train_1[targets[0]['image_id']] = np.mean(scores)\\n    \\nprint('Inference Time Used: {:.2f} seconds'.format(time.time()- time_begin))\\n\\ninfer_time_val_1 = {}\\nf2_value_val_1 = {}\\n\\ntime_begin = time.time()\\nfor ix, (image, targets) in enumerate(test_loader):\\n    preds = []\\n    gts = []\\n    t0 = time.time() \\n    outputs = model_1(image)[0]\\n    infe_time = time.time() - t0 \\n    infer_time_val_1[targets[0]['image_id']] = infe_time\\n    gts.append(targets[0]['boxes']) \\n    preds.append(outputs['boxes'].cpu().detach())\\n    scores = [calculate_score(preds, gts, iou_th) for iou_th in iou_ths]\\n    f2_value_val_1[targets[0]['image_id']] = np.mean(scores)\\n    \\nprint('Inference Time Used: {:.2f} seconds'.format(time.time()- time_begin))\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# F2 score on training data and test data for model_1, if both are Faster RCNN\n",
    "infer_time_train_1 = {}\n",
    "f2_value_train_1 = {}\n",
    "iou_ths = np.arange(0.3, 0.85, 0.05)\n",
    "\n",
    "time_begin = time.time()\n",
    "for ix, (image, targets) in enumerate(train_loader):\n",
    "    preds = []\n",
    "    gts = []\n",
    "    t0 = time.time() \n",
    "    outputs = model_1(image)[0]\n",
    "    infe_time = time.time() - t0 \n",
    "    infer_time_train_1[targets[0]['image_id']] = infe_time\n",
    "    gts.append(targets[0]['boxes']) \n",
    "    preds.append(outputs['boxes'].cpu().detach())\n",
    "    scores = [calculate_score(preds, gts, iou_th) for iou_th in iou_ths]\n",
    "    f2_value_train_1[targets[0]['image_id']] = np.mean(scores)\n",
    "    \n",
    "print('Inference Time Used: {:.2f} seconds'.format(time.time()- time_begin))\n",
    "\n",
    "infer_time_val_1 = {}\n",
    "f2_value_val_1 = {}\n",
    "\n",
    "time_begin = time.time()\n",
    "for ix, (image, targets) in enumerate(test_loader):\n",
    "    preds = []\n",
    "    gts = []\n",
    "    t0 = time.time() \n",
    "    outputs = model_1(image)[0]\n",
    "    infe_time = time.time() - t0 \n",
    "    infer_time_val_1[targets[0]['image_id']] = infe_time\n",
    "    gts.append(targets[0]['boxes']) \n",
    "    preds.append(outputs['boxes'].cpu().detach())\n",
    "    scores = [calculate_score(preds, gts, iou_th) for iou_th in iou_ths]\n",
    "    f2_value_val_1[targets[0]['image_id']] = np.mean(scores)\n",
    "    \n",
    "print('Inference Time Used: {:.2f} seconds'.format(time.time()- time_begin))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c4d7b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Time Used: 846.48 seconds\n",
      "Inference Time Used: 93.36 seconds\n"
     ]
    }
   ],
   "source": [
    "# F2 score on training data and test data for model_2\n",
    "infer_time_train_2 = {}\n",
    "f2_value_train_2 = {}\n",
    "\n",
    "time_begin = time.time()\n",
    "for ix, (image, targets) in enumerate(train_loader):\n",
    "    preds = []\n",
    "    gts = []\n",
    "    t0 = time.time() \n",
    "    outputs = model_2(image)[0]\n",
    "    infe_time = time.time() - t0 \n",
    "    infer_time_train_2[targets[0]['image_id']] = infe_time\n",
    "    gts.append(targets[0]['boxes']) \n",
    "    preds.append(outputs['boxes'].cpu().detach())\n",
    "    scores = [calculate_score(preds, gts, iou_th) for iou_th in iou_ths]\n",
    "    f2_value_train_2[targets[0]['image_id']] = np.mean(scores)\n",
    "    \n",
    "print('Inference Time Used: {:.2f} seconds'.format(time.time()- time_begin))\n",
    "\n",
    "\n",
    "infer_time_val_2 = {}\n",
    "f2_value_val_2 = {}\n",
    "\n",
    "time_begin = time.time()\n",
    "for ix, (image, targets) in enumerate(test_loader):\n",
    "    preds = []\n",
    "    gts = []\n",
    "    t0 = time.time() \n",
    "    outputs = model_2(image)[0]\n",
    "    infe_time = time.time() - t0 \n",
    "    infer_time_val_2[targets[0]['image_id']] = infe_time\n",
    "    gts.append(targets[0]['boxes']) \n",
    "    preds.append(outputs['boxes'].cpu().detach())\n",
    "    scores = [calculate_score(preds, gts, iou_th) for iou_th in iou_ths]\n",
    "    f2_value_val_2[targets[0]['image_id']] = np.mean(scores)\n",
    "    \n",
    "print('Inference Time Used: {:.2f} seconds'.format(time.time()- time_begin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11dea869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paired t-test for training set:\n",
      "Ttest_relResult(statistic=-156.216680882234, pvalue=0.0)\n",
      "Ttest_relResult(statistic=-24.33555268018417, pvalue=3.532817788643768e-123)\n",
      "Paired t-test for test set:\n",
      "Ttest_relResult(statistic=-81.651962590752, pvalue=3.793314724480224e-288)\n",
      "Ttest_relResult(statistic=-6.887899320151636, pvalue=8.70197956455147e-12)\n"
     ]
    }
   ],
   "source": [
    "# paired t-test on detection speed and F2 score \n",
    "df_t_test_train = pd.DataFrame([infer_time_train_1, infer_time_train_2, f2_value_train_1, f2_value_train_2]).T\n",
    "col_name = ['Inference_time_model_1', 'Inference_time_model_2', 'f2_model_1', 'f2_model_2']\n",
    "df_t_test_train.columns = col_name\n",
    "\n",
    "df_t_test_val = pd.DataFrame([infer_time_val_1, infer_time_val_2, f2_value_val_1, f2_value_val_2]).T\n",
    "col_name = ['Inference_time_model_1', 'Inference_time_model_2', 'f2_model_1', 'f2_model_2']\n",
    "df_t_test_val.columns = col_name\n",
    "\n",
    "print('Paired t-test for training set:')\n",
    "print(stats.ttest_rel(df_t_test_train['Inference_time_model_1'], \n",
    "                      df_t_test_train['Inference_time_model_2'], \n",
    "                      nan_policy='omit', alternative='less')) \n",
    "\n",
    "print(stats.ttest_rel(df_t_test_train['f2_model_1'], df_t_test_train['f2_model_2'], \n",
    "                      nan_policy='omit', alternative='less'))\n",
    "\n",
    "print('Paired t-test for test set:')\n",
    "print(stats.ttest_rel(df_t_test_val['Inference_time_model_1'], \n",
    "                      df_t_test_val['Inference_time_model_2'], \n",
    "                      nan_policy='omit', alternative='less')) \n",
    "\n",
    "print(stats.ttest_rel(df_t_test_val['f2_model_1'], df_t_test_val['f2_model_2'], \n",
    "                      nan_policy='omit', alternative='less'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef8ab5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference_time_model_1    0.056338\n",
      "Inference_time_model_2    0.094270\n",
      "f2_model_1                0.900007\n",
      "f2_model_2                0.979974\n",
      "dtype: float64\n",
      "--------------------------------------------------\n",
      "Inference_time_model_1    0.056696\n",
      "Inference_time_model_2    0.094454\n",
      "f2_model_1                0.887822\n",
      "f2_model_2                0.960163\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df_t_test_train.mean()) \n",
    "print('-' * 50)\n",
    "print(df_t_test_val.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b345b0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      "{} &  Inference\\_time\\_model\\_1 &  Inference\\_time\\_model\\_2 &  f2\\_model\\_1 &  f2\\_model\\_2 \\\\\n",
      "\\midrule\n",
      "1-9322 &                0.911984 &                0.093620 &    0.909091 &    1.000000 \\\\\n",
      "1-8839 &                0.065822 &                0.091413 &    0.714286 &    0.937500 \\\\\n",
      "0-4237 &                0.065822 &                0.094010 &    1.000000 &    1.000000 \\\\\n",
      "0-78   &                0.061711 &                0.093529 &    1.000000 &    1.000000 \\\\\n",
      "0-9832 &                0.075787 &                0.093728 &    1.000000 &    1.000000 \\\\\n",
      "1-4256 &                0.068812 &                0.093726 &    1.000000 &    1.000000 \\\\\n",
      "1-5853 &                0.075995 &                0.093734 &    1.000000 &    1.000000 \\\\\n",
      "1-9122 &                0.067844 &                0.093991 &    0.975758 &    0.986842 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sl4n21\\AppData\\Local\\Temp\\ipykernel_12804\\1652859269.py:1: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(df_t_test_train.head(8).iloc[:, 0:4].to_latex(index=True))\n"
     ]
    }
   ],
   "source": [
    "print(df_t_test_train.head(8).iloc[:, 0:4].to_latex(index=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
